{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인스타그램 크롤링\n",
    "- 인스타그램 게시물을 크롤링하여 벡터DB에 저장할 데이터를 구축한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "instagramAccountName = \"ugly_lovely_official\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5988991784173412"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_sleep(min_sleep = 0.01, max_sleep = 0.2):\n",
    "    random_sec = random.uniform(min_sleep, max_sleep)\n",
    "    return random_sec\n",
    "\n",
    "random_sleep(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .env\n",
    "load_dotenv()\n",
    "\n",
    "INSTAGRAM_ID = os.environ.get('INSTAGRAM_ID')\n",
    "INSTAGRAM_PASSWORD = os.environ.get('INSTAGRAM_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_element_by_xpath(driver, xpath):\n",
    "    try:\n",
    "        element = driver.find_element(By.XPATH, xpath)\n",
    "        element.click()\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking element by XPATH {xpath}: {e}\")\n",
    "\n",
    "def click_element_by_id(driver, element_id):\n",
    "    try:\n",
    "        element = driver.find_element(By.ID, element_id)\n",
    "        element.click()\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking element by ID {element_id}: {e}\")\n",
    "\n",
    "def click_element_by_class_name(driver, class_name):\n",
    "    try:\n",
    "        element = driver.find_element(By.CLASS_NAME, class_name)\n",
    "        element.click()\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking element by class name {class_name}: {e}\")\n",
    "\n",
    "def click_element_by_LINK_TEXT(driver, link_text):\n",
    "    try:\n",
    "        element = driver.find_element(By.LINK_TEXT, link_text)\n",
    "        element.click()\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking element by class name {link_text}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 브라우저 꺼짐 방지 옵션\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", ['enable-logging'])\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "# 드라이버 생성\n",
    "# 크롬 드라이버 서비스 생성\n",
    "service = ChromeService(executable_path=ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "wait = WebDriverWait(driver, 5)\n",
    "driver.implicitly_wait(time_to_wait=10)\n",
    "driver.get(\"https://instagram.com\")\n",
    "driver.implicitly_wait(time_to_wait=10)\n",
    "\n",
    "\n",
    "login_id = driver.find_element(By.CSS_SELECTOR, 'input[name=\"username\"]')\n",
    "login_id.send_keys(INSTAGRAM_ID) # 아이디 입력\n",
    "random_sleep()\n",
    "login_pwd = driver.find_element(By.CSS_SELECTOR, 'input[name=\"password\"]')\n",
    "login_pwd.send_keys(INSTAGRAM_PASSWORD) # 비번 입력\n",
    "random_sleep()\n",
    "\n",
    "login_id.send_keys(Keys.ENTER) #enter 키를 쳐주세요\n",
    "driver.implicitly_wait(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_address(driver,elements_dict) :\n",
    "    # BeautifulSoup를 사용하여 페이지 소스 파싱\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # 모든 게시글 링크 추출\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if a['href'].startswith('/p/'):\n",
    "            element = \"https://instagram.com\" + a['href']\n",
    "            elements_dict[f'{element}'] = element\n",
    "        if a['href'].startswith('/reel/'):\n",
    "            element = \"https://instagram.com\" + a['href']\n",
    "            elements_dict[f'{element}'] = element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "24\n",
      "36\n",
      "48\n",
      "60\n",
      "72\n",
      "83\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "adURL= f\"https://instagram.com/{instagramAccountName}\"\n",
    "driver.get(adURL)\n",
    "\n",
    "# 추가 게시물 확인 및 데이터 로딩\n",
    "# 페이지 로딩 후 스크롤\n",
    "SCROLL_PAUSE_TIME = 3\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "elements_dict = {}\n",
    "\n",
    "add_address(driver,elements_dict)\n",
    "\n",
    "while True:\n",
    "    # 스크롤 다운\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # 로딩 대기\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    add_address(driver,elements_dict)\n",
    "    print(len(elements_dict))\n",
    "\n",
    "\n",
    "    # 새로운 높이 계산\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # 높이가 변하지 않으면 스크롤 종료\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "iter_items = iter(elements_dict.items())\n",
    "next(iter_items)\n",
    "second_key, second_value = next(iter_items)\n",
    "driver.get(elements_dict[second_value])\n",
    "\n",
    "print(len(elements_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_count_in_posts(driver):\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    elements_num = len(soup.select('div._acnb'))\n",
    "    return elements_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def convert_timestamp(timestamp_str):\n",
    "    # 입력된 timestamp_str을 datetime 객체로 파싱\n",
    "    dt = datetime.strptime(timestamp_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    \n",
    "    # 원하는 포맷으로 변환하여 반환\n",
    "    return dt.strftime('%Y.%m.%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import mimetypes\n",
    "\n",
    "def download_image_from_url(image_src, index, save_dir='./images'):\n",
    "    save_as = os.path.join(save_dir, f\"{index}.jpg\")\n",
    "    \n",
    "    # 이미지 다운로드\n",
    "    response = requests.get(image_src)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_as, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        # Content-Type 헤더를 이용하여 파일 확장자 추출\n",
    "        content_type = response.headers.get('content-type')\n",
    "        ext = mimetypes.guess_extension(content_type)\n",
    "        if ext:\n",
    "            # 다운로드한 파일의 확장자를 변경\n",
    "            new_file_name = save_as.replace(\".jpg\", ext)\n",
    "            os.rename(save_as, new_file_name)\n",
    "            save_as = new_file_name\n",
    "        \n",
    "        print(f\"이미지 다운로드 완료: {save_as}\")\n",
    "        return save_as\n",
    "    else:\n",
    "        print(f\"이미지 다운로드 실패: HTTP 상태 코드 {response.status_code}\")\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_text(driver):\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Extract the time element\n",
    "    time_element = soup.find('time', class_='xsgj6o6')\n",
    "    datetime = convert_timestamp(time_element['datetime'])\n",
    "\n",
    "    # Extract the content\n",
    "    content_element = soup.find('span', class_='x193iq5w xeuugli x1fj9vlw x13faqbe x1vvkbs xt0psk2 x1i0vuye xvs91rp xo1l8bm x5n08af x10wh9bi x1wdrske x8viiok x18hxmgj')\n",
    "    \n",
    "    # Extract the hashtags\n",
    "    hashtags = [a.get_text() for a in content_element.find_all('a')]\n",
    "    \n",
    "    # Remove <a> tags from the content\n",
    "    for a in content_element.find_all('a'):\n",
    "        a.decompose()\n",
    "\n",
    "    content = ''.join(content_element.stripped_strings)\n",
    "\n",
    "\n",
    "    # Construct the JSON data\n",
    "    data = {\n",
    "        '생성시간': datetime,\n",
    "        '게시글내용': content,\n",
    "        '해쉬태그': hashtags\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def dict_to_csv(data, csv_filename='output.csv'):\n",
    "    # 딕셔너리의 키 값을 CSV 파일의 헤더로 사용\n",
    "    fields = list(data[0].keys())  # Assuming all dictionaries in data have the same keys\n",
    "    \n",
    "    # CSV 파일 쓰기 모드로 열기\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['Index'] + fields)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # 각 데이터를 순회하며 CSV 파일에 쓰기\n",
    "        for idx, entry in enumerate(data, start=1):\n",
    "            row = {'Index': idx}\n",
    "            row.update(entry)\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f'CSV 파일 \"{csv_filename}\" 저장 완료.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "이미지 다운로드 완료: ./images\\0.jpg\n",
      "0\n",
      "이미지 다운로드 완료: ./images\\1.jpg\n",
      "2\n",
      "이미지 다운로드 완료: ./images\\2.jpg\n",
      "0\n",
      "이미지 다운로드 완료: ./images\\3.jpg\n",
      "0\n",
      "이미지 다운로드 완료: ./images\\4.jpg\n",
      "CSV 파일 \"output.csv\" 저장 완료.\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# 각 게시글의 텍스트와 이미지를 추출\n",
    "data = []\n",
    "xpath = \"//img[@class='x5yr21d xu96u03 x10l6tqk x13vifvy x87ps6o xh8yej3']\"\n",
    "for index, value in enumerate(list(elements_dict)[:5]):\n",
    "    \n",
    "    driver.get(value)\n",
    "    \n",
    "    num=get_image_count_in_posts(driver)\n",
    "    print(num)\n",
    "    \n",
    "    # 게시글 모든 사진 가져오기\n",
    "    # for i in range(1,num+1):\n",
    "    #     driver.get(f\"{value}?img_index={i}\")\n",
    "\n",
    "    image_element = WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, xpath))\n",
    "        )\n",
    "    \n",
    "    image_src = image_element.get_attribute(\"src\")\n",
    "    label = str(index)\n",
    "    save_img = download_image_from_url(image_src, label)\n",
    "\n",
    "    post_text = get_post_text(driver)\n",
    "\n",
    "    post_text['img_path']= save_img\n",
    "    data.append(post_text)\n",
    "    \n",
    "dict_to_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
